{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gen\n",
    "using GenViz\n",
    "using Statistics\n",
    "include(\"DistributionsBacked.jl\")\n",
    "using PyPlot\n",
    "using AdvancedHMC\n",
    "using Distributions\n",
    "\n",
    "const my_normal = DistributionsBacked{Float64}((mu, sigma) -> \n",
    "                            Distributions.Normal(mu, sigma), [true, true], true)\n",
    "const my_unif = DistributionsBacked{Float64}((lo, hi) -> \n",
    "                            Distributions.Uniform(lo, hi), [true, true], true)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function corbiv_model() #correlated bivariate normal\n",
    "    ρ ~ my_unif(-1., 1.)\n",
    "    x ~ my_normal(0., 1.)\n",
    "    y ~ my_normal(ρ*x, sqrt(1. - ρ^2))\n",
    "end\n",
    "\n",
    "@gen function cormiv_model() #correlated bivariate normal\n",
    "    ρ ~ my_unif(0., 1.) #correlation between x and y\n",
    "    ρ2 ~ my_unif(0., ρ) #correlation between z and (x or y) \n",
    "    zy_cond_zx = (1-ρ2^2)*ρ2\n",
    "    x ~ my_normal(0., 1.)\n",
    "    y ~ my_normal(ρ*x, sqrt(1. - ρ^2))\n",
    "    z ~ my_normal(ρ2*x + zy_cond_zx*y, sqrt(1. - ρ2^2 - zy_cond_zx^2))\n",
    "end\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function constrainρ(ρ::Float64)\n",
    "    constraints = Gen.choicemap()\n",
    "    constraints[:ρ] = ρ\n",
    "    constraints\n",
    "end\n",
    "\n",
    "function constrainρ2(ρ::Float64, ρ2, z)\n",
    "    constraints = Gen.choicemap()\n",
    "    constraints[:ρ] = ρ\n",
    "    constraints[:ρ2] = ρ2\n",
    "    constraints[:z] = z\n",
    "    \n",
    "    \n",
    "    constraints\n",
    "end\n",
    "\n",
    "function mcmc_inference(ρ, num_iters, update, selection)\n",
    "    observation = constrainρ(ρ)\n",
    "    (trace, _) = generate(corbiv_model, (), observation)\n",
    "    samples = Array{Float64}(undef,num_iters,2)\n",
    "    for i=1:num_iters\n",
    "        trace = update(trace, selection)\n",
    "        ch = get_choices(trace)\n",
    "        samples[i,1] = ch[:x]\n",
    "        samples[i,2] = ch[:y]\n",
    "    end\n",
    "    samples\n",
    "end\n",
    "\n",
    "function mcmc_m_inference(ρ, ρ2, z, num_iters, update)\n",
    "    observation = constrainρ2(ρ, ρ2, z)\n",
    "    (trace, _) = generate(cormiv_model, (), observation)\n",
    "    samples = Array{Float64}(undef,num_iters,2)\n",
    "    for i=1:num_iters\n",
    "        trace = update(trace)\n",
    "        ch = get_choices(trace)\n",
    "        samples[i,1] = ch[:x]\n",
    "        samples[i,2] = ch[:y]\n",
    "    end\n",
    "    samples\n",
    "end\n",
    "\n",
    "function block_mh(tr, selection)\n",
    "    (tr, _) = mh(tr, select(:x, :y))\n",
    "    tr\n",
    "end\n",
    "\n",
    "\n",
    "function simple_hmc(tr, selection)\n",
    "    (tr, _) = hmc(tr, select(:x, :y))\n",
    "    tr\n",
    "end\n",
    "\n",
    "\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 10_000\n",
    "show = 5\n",
    "ρ = -.5\n",
    "samps = mcmc_inference(ρ, iters, block_mh, select(:x,:y))\n",
    "samps[(iters-show+1):iters,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 100\n",
    "show = 5\n",
    "ρ = .8\n",
    "samps = mcmc_inference(ρ, iters, simple_hmc)\n",
    "samps[(iters-show+1):iters,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "println(mean(samps))\n",
    "println(cor(samps[:,1],samps[:,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disable AdvancedHMC's NUTS logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Logging\n",
    "using LoggingExtras\n",
    "\n",
    "function ignore_sampling_filter(log_args)\n",
    "    !(occursin(\"sampling steps\",log_args.message) || occursin(\"adapation steps\",log_args.message))\n",
    "end\n",
    "logger = ActiveFilteredLogger(ignore_sampling_filter, global_logger())\n",
    "\n",
    "\n",
    "if !(@isdefined old_logger) #do this only once\n",
    "    old_logger = global_logger(logger)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_nuts (generic function with 4 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function my_nuts(trace, selection, \n",
    "        n_postadapt_steps = 2,\n",
    "        n_adapts = 1,\n",
    "        initial_ϵ_reduce_fac = 10)\n",
    "    \n",
    "    n_NUTS_steps = n_postadapt_steps + n_adapts\n",
    "    \n",
    "    filtered_choices = get_selected(get_choices(trace), selection)\n",
    "    cur_xy = to_array(filtered_choices, Float64)\n",
    "    dimension = length(cur_xy)\n",
    "    metric = DiagEuclideanMetric(dimension)\n",
    "    \n",
    "    retval_grad = nothing #accepts_output_grad(get_gen_fn(trace)) ? zero(get_retval(trace)) : nothing\n",
    "    \n",
    "    function update_xy(val)\n",
    "        extra_constraints = from_array(filtered_choices, val)\n",
    "        update(trace, (), (NoChange(),), extra_constraints)\n",
    "    end\n",
    "    \n",
    "    function val_to_lp_plus_c(val)\n",
    "        (new_trace, weight, discard, retdiff) = update_xy(val)\n",
    "        weight\n",
    "    end\n",
    "    \n",
    "    function val_to_grad(val)\n",
    "        (new_trace, weight, discard, retdiff) = update_xy(val)\n",
    "        (retval_grad_out, values_trie, gradient_trie) = choice_gradients(new_trace, selection, retval_grad)\n",
    "        grad = [gradient_trie[:x], gradient_trie[:y]]\n",
    "        (weight, grad)\n",
    "    end\n",
    "    \n",
    "    # Define a Hamiltonian system, using metric defined globally above\n",
    "    hamiltonian = Hamiltonian(metric, val_to_lp_plus_c, val_to_grad)\n",
    "    \n",
    "    # Define a leapfrog solver, with initial step size chosen heuristically\n",
    "    initial_ϵ = find_good_stepsize(hamiltonian, cur_xy) ./ initial_ϵ_reduce_fac\n",
    "    integrator = Leapfrog(initial_ϵ)\n",
    "    \n",
    "    # Define an HMC sampler, with the following components\n",
    "    #   - multinomial sampling scheme,\n",
    "    #   - generalised No-U-Turn criteria, and\n",
    "    #   - windowed adaption for step-size and diagonal mass matrix\n",
    "    proposal = NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)\n",
    "    adaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))\n",
    "\n",
    "    # Run the sampler to draw samples from the specified Gaussian, where\n",
    "    #   - `samples` will store the samples\n",
    "    #   - `stats` will store diagnostic statistics for each sample\n",
    "    \n",
    "    samples, stats = sample(hamiltonian, proposal, cur_xy, n_NUTS_steps, adaptor, n_adapts; progress=false)\n",
    "    \n",
    "    #println(samples[3])\n",
    "    \n",
    "    \n",
    "    (new_trace, weight, discard, retdiff) = update_xy(samples[n_NUTS_steps])\n",
    "    new_trace\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×2 Array{Float64,2}:\n",
       " -0.170998   0.0697753\n",
       " -0.170276   0.0757997\n",
       " -0.450133  -0.684956\n",
       " -0.863553  -0.637876\n",
       " -0.87275   -0.666354"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iters = 200\n",
    "show = 5\n",
    "ρ = .99\n",
    "\n",
    "\n",
    "samps = mcmc_inference(ρ, iters, my_nuts, select(:x,:y))\n",
    "samps[(iters-show+1):iters,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6782889835811934\n",
      "0.96059601\n"
     ]
    }
   ],
   "source": [
    "println(cor(samps[1:iters-1,1],samps[2:iters,1])) #serial correlation; lower is better\n",
    "println(ρ^4) #for comparison, gibbs would be ρ² for each step; ρ⁴ for two steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "jl:light,ipynb"
  },
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
