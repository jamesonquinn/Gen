{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell may take a few seconds to run.\n",
    "using Gen\n",
    "using PyPlot\n",
    "using StatsBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIDE_compare (generic function with 1 method)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second(x) = x[2] #used to pull logweight from (trace, logweight)\n",
    "\n",
    "function AIDE_compare(f, g, #generative functions to compare. Should return the important value.\n",
    "        input, #any input to generative functions; must be same for both\n",
    "        ret_to_f, ret_to_g, #take a return value from either generative, and produce constraints for one generative\n",
    "        Nf, Ng, #Number of traces to draw from each\n",
    "        Mf, Mg  #Number of runs of each to use to estimate score\n",
    "    )\n",
    "    cm = choicemap()\n",
    "    f_vals = [Gen.get_retval(generate(f, input, cm)[1]) for i in 1:Nf]\n",
    "    g_vals = [Gen.get_retval(generate(g, input, cm)[1]) for i in 1:Ng]\n",
    "    \n",
    "    f_scores_of_g = [mean(\n",
    "                        [second(generate(f, input, ret_to_f(trace))) \n",
    "                            for j in 1:Mf]) \n",
    "                    for trace in g_vals]\n",
    "    g_scores_of_f = [mean(\n",
    "                        [second(generate(g, input, ret_to_g(trace)))\n",
    "                            for j in 1:Mf])\n",
    "                    for trace in f_vals]\n",
    "    f_scores_of_f = [mean(\n",
    "                        [second(generate(f, input, ret_to_f(trace)))\n",
    "                            for j in 1:Mf]) \n",
    "                    for trace in f_vals]\n",
    "    g_scores_of_g = [mean(\n",
    "                        [second(generate(g, input, ret_to_g(trace)))\n",
    "                            for j in 1:Mf]) \n",
    "                    for trace in g_vals]\n",
    "    \n",
    "    KLfg = mean(f_scores_of_f) - mean(g_scores_of_f)\n",
    "    KLgf = mean(g_scores_of_g) - mean(f_scores_of_g)\n",
    "    \n",
    "    KLfg + KLgf\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ret_to_analytic_constraint (generic function with 1 method)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@gen function binom_prior(α, β)\n",
    "    p ~ beta(α, β)\n",
    "    p\n",
    "end\n",
    "\n",
    "@gen function basic_binom(n, α, β)\n",
    "    p ~ beta(α, β)\n",
    "    h ~ binom(n, p)\n",
    "end\n",
    "\n",
    "function importance_cond_binom_for(samps)\n",
    "    @gen function i_c_b(h, n, α = 1., β = 1.)\n",
    "        #@warn \"i_c_b\" samps h n α\n",
    "        constraint = choicemap((:h, h))\n",
    "        logweights = Vector{Float64}(undef, samps)\n",
    "        ps = Vector{Float64}(undef, samps)\n",
    "        for s in 1:samps\n",
    "            p = @trace(binom_prior(α, β),:data => s)\n",
    "            constraint[:p] = p\n",
    "            ps[s] = p\n",
    "            (_, logweights[s]) = generate(basic_binom, (n, α, β), constraint)\n",
    "                    #in this simple case, this is just a more-complicated way of calling\n",
    "                        #Distributions.Binomial(...).logpdf. However, this pattern generalizes\n",
    "                        #better to arbitrary generative functions.\n",
    "        end\n",
    "        logweights .-= max(logweights...)\n",
    "        weights = exp.(logweights)\n",
    "        weights ./= sum(weights)\n",
    "        whichOne ~ categorical(weights)\n",
    "        p = ps[whichOne]\n",
    "        p\n",
    "    end\n",
    "end\n",
    "\n",
    "@gen function analytic_cond_binom(h, n, α = 1., β = 1.)\n",
    "    p ~ beta(α + h, β + n - h)\n",
    "    p\n",
    "end\n",
    "\n",
    "function ret_to_importance_constraint(p)\n",
    "    constraint = choicemap((:whichOne, 1))\n",
    "    constraint[:data => 1 => :p] = p\n",
    "    constraint\n",
    "end\n",
    "\n",
    "function ret_to_analytic_constraint(p)\n",
    "    choicemap((:p, p))\n",
    "end\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24809589999296622"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_cond_binom_for(5)(0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 8.392623797620825\n",
       " 1.5453535899054787\n",
       " 1.9898080391574924\n",
       " 0.8577957749392957\n",
       " 1.0811169196341945"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = 5\n",
    "ms = 5\n",
    "\n",
    "[AIDE_compare(importance_cond_binom_for(i), analytic_cond_binom, #generative functions to compare. Should return the important value.\n",
    "        (0,10), #any input to generative functions; must be same for both\n",
    "        ret_to_importance_constraint, ret_to_analytic_constraint, #take a return value from either generative, and produce constraints for one generative\n",
    "        ns, ns, #Number of traces to draw from each\n",
    "        ms, ms  #Number of runs of each to use to estimate score\n",
    "    ) for i in 1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       " 7.407463144199239\n",
       " 2.1709149701587904\n",
       " 1.3327715571824372\n",
       " 0.7161851858275738\n",
       " 0.6566965256823987"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns = 50\n",
    "ms = 50\n",
    "\n",
    "[AIDE_compare(importance_cond_binom_for(i), analytic_cond_binom, #generative functions to compare. Should return the important value.\n",
    "        (0,10), #any input to generative functions; must be same for both\n",
    "        ret_to_importance_constraint, ret_to_analytic_constraint, #take a return value from either generative, and produce constraints for one generative\n",
    "        ns, ns, #Number of traces to draw from each\n",
    "        ms, ms  #Number of runs of each to use to estimate score\n",
    "    ) for i in 1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "jl:light,ipynb"
  },
  "kernelspec": {
   "display_name": "Julia 1.4.0",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
